{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNrNN0hc+wHin8aTnzyaiAn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eshan-K-Kaushal/Voice_Chatbot/blob/main/T5_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WAyOGUDU61F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install transformers\n",
        "!pip install sentencepiece==0.1.91\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from termcolor import colored\n",
        "import textwrap\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration, \\\n",
        "    AdamW\n",
        "\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name) # using t5 since distilbert dont work well with the complex question - answer pipeline\n",
        "\n",
        "with Path('/content/context.json').open() as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "def extract_questions_and_answers(path):\n",
        "    with Path(path).open() as json_file:\n",
        "        data = json.load(json_file)\n",
        "    questions = data[\"materiel\"]\n",
        "    data_row = []\n",
        "    for question in questions:\n",
        "        context = question[\"context\"]\n",
        "        for question_and_answers in question[\"qas\"]:\n",
        "            q = question_and_answers[\"question\"]\n",
        "            a = question_and_answers[\"answers\"]\n",
        "\n",
        "            for answer in a:\n",
        "                answer_text = answer[\"text\"]\n",
        "                answer_start = answer[\"answer_start\"]\n",
        "                answer_end = answer_start + len(answer_text) # get the end of the answer so the model knows what's going on\n",
        "\n",
        "                data_row.append({\n",
        "                    \"question\" : q,\n",
        "                    \"context\" : context,\n",
        "                    \"answer_text\": answer_text,\n",
        "                    \"answer_start\": answer_start,\n",
        "                    \"answer_end\":answer_end\n",
        "                })\n",
        "    return pd.DataFrame(data_row) # make a data frome that has all the info in it\n",
        "\n",
        "\n",
        "df = extract_questions_and_answers('/content/context.json')\n",
        "\n",
        "class dataset_creation(Dataset):\n",
        "    def __init__(self, data, tokenizer, source_max_token_len = 396,\n",
        "                 target_max_token_length = 64):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.source_max_token_len = source_max_token_len\n",
        "        self.target_max_token_length = target_max_token_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index:int):\n",
        "        data_row = self.data.iloc[index]\n",
        "\n",
        "        source_encoding = tokenizer(\n",
        "            data_row['question'],\n",
        "            data_row['context'][0], # put zero since the data_row[\"context\"] is a list and we want just the first string of the list\n",
        "            max_length=self.source_max_token_len, # max length is being given so that all the sentences can be of the same length - important to pad for training purposes - since the model takes a fixed size\n",
        "            padding=\"max_length\",  # padding equal to the max_length\n",
        "            truncation=\"only_second\",  # only truncate the context - since we only want it till the answer or upto the answer\n",
        "            return_attention_mask=True, # attention mask can be fed to the model for better efficiency during training - this masks helps find out the weightage of a word wrt a sentence that came before or the context of the question\n",
        "            add_special_tokens=True, # research on it!!!!!!\n",
        "            return_tensors=\"pt\" # return the tensors in a pytorch format\n",
        "        )\n",
        "\n",
        "        target_encoding = tokenizer(\n",
        "            data_row['answer_text'],\n",
        "            max_length=self.target_max_token_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        labels = target_encoding[\"input_ids\"]\n",
        "        labels[labels == 0] = -100\n",
        "\n",
        "        return dict(\n",
        "            question = data_row[\"question\"],\n",
        "            context = data_row[\"context\"],\n",
        "            answer_text = data_row[\"answer_text\"],\n",
        "            input_ids=source_encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask = source_encoding[\"attention_mask\"].flatten(),\n",
        "            labels=labels.flatten()\n",
        "        )\n",
        "\n",
        "sample_dataset = dataset_creation(df, tokenizer)\n",
        "\n",
        "'''\n",
        "for d in sample_dataset:\n",
        "    print(d[\"question\"])\n",
        "    print(d[\"answer_text\"])\n",
        "    print(d[\"input_ids\"][:20])\n",
        "    print(d[\"labels\"][:20])\n",
        "'''\n",
        "\n",
        "\n",
        "###TESTING!!!###\n",
        "#pd.set_option('max_columns', 5)\n",
        "#print(df)\n",
        "#print(df['context'][0])\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.05)\n",
        "print(train_df.shape, val_df.shape)\n",
        "\n",
        "# SET THE NUMBER OF WORKERS HERE\n",
        "\n",
        "class data_module(pl.LightningDataModule):\n",
        "    def __init__(self, train_df, test_df, tokenizer, batch_size = 1,\n",
        "                 source_max_token_len=396,\n",
        "                 target_max_token_length=64\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.source_max_token_len = source_max_token_len\n",
        "        self.target_max_token_length = target_max_token_length\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = dataset_creation(\n",
        "            self.train_df, self.tokenizer, self.source_max_token_len, self.target_max_token_length\n",
        "        )\n",
        "\n",
        "        self.test_dataset = dataset_creation(\n",
        "            self.test_df, self.tokenizer, self.source_max_token_len, self.target_max_token_length\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset, batch_size=1, num_workers=2\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset, batch_size=1, num_workers=2\n",
        "        )\n",
        "\n",
        "BATCH_SIZE = 2 # BATCH SIZE\n",
        "N_EPOCH = 60 # N_EPOCHS\n",
        "\n",
        "data_module_use = data_module(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
        "data_module_use.setup()\n",
        "\n",
        "#model = T5ForConditionalGeneration.from_pretrained(model_name, return_dict = True)\n",
        "\n",
        "class QA_Model(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name, return_dict=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def val_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        loss, outputs = self.forward(input_ids, attention_mask, labels)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.model.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = QA_Model()\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints\", filename=\"best-checkpoint\",\n",
        "                                      save_top_k=1, verbose=True, monitor=\"val_loss\",\n",
        "                                      mode=\"min\")\n",
        "\n",
        "trainer = pl.Trainer(checkpoint_callback=checkpoint_callback,\n",
        "                     max_epochs=N_EPOCH, gpus=1, progress_bar_refresh_rate=30)\n",
        "\n",
        "trainer.fit(model, data_module_use)"
      ],
      "metadata": {
        "id": "ezN5FuiZXzO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = QA_Model.load_from_checkpoint(\"/content/epoch=36-step=444.ckpt\")\n",
        "trained_model.freeze()"
      ],
      "metadata": {
        "id": "tBYnU5fxYAZj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question): \n",
        "\n",
        "  source_encoding = tokenizer(\n",
        "            question['question'],\n",
        "            question['context'][0], # put zero since the data_row[\"context\"] is a list and we want just the first string of the list\n",
        "            max_length=1024, # max length is being given so that all the sentences can be of the same length - important to pad for training purposes - since the model takes a fixed size\n",
        "            padding=\"max_length\",  # padding equal to the max_length\n",
        "            truncation=\"only_second\",  # only truncate the context - since we only want it till the answer or upto the answer\n",
        "            return_attention_mask=True, # attention mask can be fed to the model for better efficiency during training - this masks helps find out the weightage of a word wrt a sentence that came before or the context of the question\n",
        "            add_special_tokens=True, # research on it!!!!!!\n",
        "            return_tensors=\"pt\" # return the tensors in a pytorch format\n",
        "        )\n",
        "  generated_ids = trained_model.model.generate(\n",
        "          input_ids = source_encoding[\"input_ids\"],\n",
        "          attention_mask = source_encoding[\"attention_mask\"],\n",
        "          num_beams = 4, # how many beam searches you want to have\n",
        "          max_length = 200,\n",
        "          repetition_penalty = 2.5,\n",
        "          length_penalty = 1.0,\n",
        "          early_stopping=True,\n",
        "          use_cache=True\n",
        "      )\n",
        "\n",
        "  pred = [\n",
        "          tokenizer.decode(generated_id, skip_special_tokens=True, \n",
        "                           clean_up_tokenization_spaces=True)\n",
        "          for generated_id in generated_ids\n",
        "  ]\n",
        "\n",
        "  return \"\".join(pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "jiWH2akPY6sg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom function for the generation of the answers on the custom questions from the user\n",
        "def generate_answer_custom(question): \n",
        "\n",
        "  source_encoding = tokenizer(\n",
        "            question,\n",
        "            train_df['context'][0][0], # put zero since the data_row[\"context\"] is a list and we want just the first string of the list\n",
        "            max_length=700, # max length is being given so that all the sentences can be of the same length - important to pad for training purposes - since the model takes a fixed size\n",
        "            padding=\"max_length\",  # padding equal to the max_length\n",
        "            truncation=\"only_second\",  # only truncate the context - since we only want it till the answer or upto the answer\n",
        "            return_attention_mask=True, # attention mask can be fed to the model for better efficiency during training - this masks helps find out the weightage of a word wrt a sentence that came before or the context of the question\n",
        "            add_special_tokens=True, # research on it!!!!!!\n",
        "            return_tensors=\"pt\" # return the tensors in a pytorch format\n",
        "        )\n",
        "  generated_ids = trained_model.model.generate(\n",
        "          input_ids = source_encoding[\"input_ids\"],\n",
        "          attention_mask = source_encoding[\"attention_mask\"],\n",
        "          num_beams = 1,\n",
        "          max_length = 100,\n",
        "          repetition_penalty = 2.5,\n",
        "          length_penalty = 1.0,\n",
        "          early_stopping=True,\n",
        "          use_cache=True\n",
        "      )\n",
        "\n",
        "  pred = [\n",
        "          tokenizer.decode(generated_id, skip_special_tokens=True, \n",
        "                           clean_up_tokenization_spaces=True)\n",
        "          for generated_id in generated_ids\n",
        "  ]\n",
        "\n",
        "  return \"\".join(pred)"
      ],
      "metadata": {
        "id": "RHnnLSZqZf04"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample_question_1 = 'how much do you earn?'\n",
        "generate_answer_custom(sample_question_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7qjnMLSZaXUQ",
        "outputId": "a0198468-e4c5-4db7-89a6-50ebb3504dbd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how much do you earn?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample_question = train_df.iloc[6]\n",
        "sample_question[\"question\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HrDBgRiTjd2R",
        "outputId": "bbe2cc02-fcc9-4f3b-bd1e-973607cc5a79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are your views on formal education?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer(sample_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "eOtu5Wp7a6qe",
        "outputId": "dcd613b6-b6a5-4bd6-d6d9-341bb6ef1957"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Most Chicanos claim that Experiences are the best education and although experiences and I agree with them but I want, I'd rather see a total educated man that has experienced working with the people and the people traits, but also what you call a formal education. By going to school and reading and researching and learning. That's what 99.9 percent of people do. I only heard rumors, and then those rumors became true, and the city became unlivable due to opportunity and safety.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_question[\"answer_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "tDHFIHgLfBaW",
        "outputId": "d206664a-d11d-4c2f-b16b-f70c220ede1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Most Chicanos claim that Experiences are the best education and although experiences and I agree with them but I want, I'd rather see a total educated man that has experienced working with the people and the people traits, but also what you call a formal education. By going to school and reading and researching and learning. That to me is a total education and knowing two languages and knowing how to read and write both languages to be the best of one's ability. That's a total man to me. And not only that a Chicano knows his experiences in life, this does not complete a total man and therefore this is my idea, so although I dropped out of school, school continued. That's why I educated myself after I was forced to leave school, because I knew I could work as hard as any man around from sunup to sundown and just like my dad, nobody could beat him at work.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['context'][0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "S_TnIDSqiN5S",
        "outputId": "cd391b03-19f0-4299-f526-ee30c376c7f2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"My name is Juan Carlos and I was born the 20th of October, 1986, in a small village in Piedras Negras, Coahuila, Mexico. I lived there most of my life until I was approximately twenty-five years old, then I immigrated here. I migrated to the United States due to cartel and a family member who took a small loan and was unable to pay it back. Cartel see the entire family as part of that debt, so it was unsafe for myself, wife, and daughter. So we made the trip here. Growing up was kind of like a very boring version of the Disney film Encanto. Everybody knew each other. We all lived the same, and all cared about each other, even if there was small city gossip. The real, non Disney magic happened every night when everybody tuned their radio to the same frequency and the city danced. The music went away when the cartel became strong, and that happened when I was about 16. I had very little connection with the cartel. We paid our taxes for the most part, and stayed clear. That’s what 99.9 percent of people do. I only heard rumors, and then those rumors became true, and the city became unlivable due to opportunity and safety. At approximately twenty-two years old I sadly needed to drop out due to an illness in my family. My father passed, and my mother could not feed all the mouths in our home. There I took up working as a mechanic on old trucks, which I found fulfilling and just as intellectually stimulating as my time in chemical engineering. Working with your hands sometimes connects you with your mind more than you think! I had worked in Mexico for minor jobs for approximately two years after I dropped out of college. I heard that there were more opportunities in United States working with Celanese Corporation of America which is a big, large company here in the United States. People told me that if I crossed the boarder there would be no issues for an engineer like myself to do small jobs and it would be safe. Sadly there was no work when I arrived. So I did what most migrants do, working fields, delivering goods, working as a roofer. Any small job that pays the bills for my family. But it is like waves, they come and go, but luckily are consistent… even if they give no benefits. I work very hard. I wake up at 5am every day to do odd jobs, and finish work at 7pm. Living on 19 thousand dollars a year gives me very little room for free time. I just work, and have Sundays with my daughter Rose. Most Chicanos claim that Experiences are the best education and although experiences and I agree with them but I want, I'd rather see a total educated man that has experienced working with the people and the people traits, but also what you call a formal education. By going to school and reading and researching and learning. That to me is a total education and knowing two languages and knowing how to read and write both languages to be the best of one's ability. That's a total man to me. And not only that a Chicano knows his experiences in life, this does not complete a total man and therefore this is my idea, so although I dropped out of school, school continued. That's why I educated myself after I was forced to leave school, because I knew I could work as hard as any man around from sunup to sundown and just like my dad, nobody could beat him at work. Well it is me, my daughter rose who is eight, my wife who is named Carla who is my age, here in the USA. My father died while I attended college, and my Mother Violeta is in Mexico with the rest of my family. Sadly, I cannot see them because of immigration.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}